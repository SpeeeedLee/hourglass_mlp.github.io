<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <!-- 載入 Bootstrap Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">
    
    <!-- 載入 MathJax -->
     <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <title>Rethinking the Shape Convention of an MLP</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        
        header {
            background: white;
            padding: 60px 40px;
            border-radius: 16px;
            margin-bottom: 30px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.1);
            text-align: center;
        }
        
        h1 {
            font-size: 2.5em;
            color: #2c3e50;
            margin-bottom: 20px;
            line-height: 1.2;
        }
        
        .authors {
            font-size: 1.1em;
            color: #555;
            margin-bottom: 10px;
        }
        
        .affiliation {
            color: #777;
            font-size: 0.95em;
            margin-bottom: 20px;
        }
        
        .section {
            background: white;
            padding: 40px;
            border-radius: 16px;
            margin-bottom: 30px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.1);
        }
        
        h2 {
            color: #667eea;
            font-size: 2em;
            margin-bottom: 25px;
            padding-bottom: 10px;
            border-bottom: 3px solid #667eea;
        }
        
        h3 {
            color: #764ba2;
            font-size: 1.4em;
            margin: 30px 0 15px 0;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
            line-height: 1.8;
        }
                
        .figure {
            text-align: center;
            margin: 40px 0;
        }
        
        .figure img {
            max-width: 100%;
            height: auto;
            border-radius: 12px;
            box-shadow: 0 5px 20px rgba(0,0,0,0.15);
            background: #f0f0f0;
            min-height: 200px;
        }
        
        .figure-caption {
            margin-top: 15px;
            color: #666;
            font-size: 0.95em;
            font-style: italic;
        }
        
        .highlight-box {
            background: linear-gradient(135deg, #667eea15, #764ba215);
            border-left: 5px solid #667eea;
            padding: 20px;
            margin: 25px 0;
            border-radius: 8px;
        }
        
        .contributions {
            background: #fff9e6;
            padding: 25px;
            border-radius: 12px;
            border-left: 5px solid #ffc107;
            margin: 25px 0;
        }
        
        ul {
            margin-left: 20px;
            margin-top: 10px;
        }
        
        li {
            margin: 10px 0;
            line-height: 1.7;
        }
        
        .result-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }
        
        .result-item {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            border-left: 4px solid #667eea;
        }
        
        .result-item h4 {
            color: #667eea;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 0.9em;
        }
        
        th, td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }
        
        th {
            background: #667eea;
            color: white;
            font-weight: 600;
        }
        
        tr:hover {
            background: #f5f5f5;
        }
        
        footer {
            display: flex;
            justify-content: center;
            align-items: center;
            color: white;
            font-size: 1.0em;
        }
        
        @media (max-width: 768px) {
            h1 {
                font-size: 1.8em;
            }
            
            .section {
                padding: 25px;
            }
            
            header {
                padding: 40px 25px;
            }
            .hero-buttons .custom-btn {
                padding: 0.4rem 0.8rem !important;
                font-size: 14px !important;
                margin: 0.2rem !important;
            }
        }
        
        code {
            background-color: #f8f9fa;
            color: #2d2d2d;
            padding: 2px 6px;
            border-radius: 4px;
            font-weight: 500;
            font-family: 'Fira Code', monospace;
            font-size: 0.95em;
            letter-spacing: 0.5px;
            border: 1px solid #e9ecef;
        }
        
        .custom-btn {
            display: inline-block;
            padding: 0.5rem 1.2rem;
            margin: 0.3rem;
            font-size: 16px;
            text-decoration: none;
            color: white;
            background: linear-gradient(135deg, #667eea, #764ba2);
            border-radius: 8px;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);
        }

        .custom-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(102, 126, 234, 0.4);
        }

        .custom-btn i {
            margin-right: 5px;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Rethinking the Shape Convention of an MLP</h1>
            <div class="authors">
                Meng-Hsi Chen<sup>1*</sup>, Yu-Ang Lee<sup>1,2*</sup>, Feng-Ting Liao<sup>1</sup>, Da-shan Shiu<sup>1</sup>
            </div>
            <div class="affiliation">
                <sup>1</sup>MediaTek Research &nbsp;&nbsp;&nbsp; <sup>2</sup>National Taiwan University<br>
                <sup>*</sup>Equal contribution
            </div>
        
            <div class="hero-buttons" style="text-align:center; margin-top: 1rem;">
                <a href="https://arxiv.org/abs/2510.01796" class="custom-btn">
                    <i class="bi bi-file-pdf"></i> Paper
                </a>
                <a href="" class="custom-btn">
                    <i class="bi bi-github"></i> Code
                </a>
                <a href="https://huggingface.co/papers/2510.01796" class="custom-btn">
                    <img src="images/huggingface.svg" alt="HuggingFace" style="height: 20px; width: auto; margin-right: 5px; vertical-align: middle;"> HuggingFace
                </a>
            </div>
        </header>


        <!-- TL;DR -->
        <div class="section">
            <h2>TL;DR</h2>
            
            <div class="highlight-box" style="background: linear-gradient(135deg, #667eea10, #764ba210); border-left: 4px solid #667eea;">
                <p style="font-size: 1.1em; font-weight: 500; color: #2c3e50; margin-bottom: 20px;">
                    <strong>Challenging conventional MLP design:</strong> Wide-narrow-wide "Hourglass" MLPs consistently outperform traditional narrow-wide-narrow architectures with fewer parameters across multiple generative tasks.
                </p>
            </div>

            <p>
                This paper inverts the standard MLP paradigm. Instead of placing skip connections at narrow input/output dimensions with processing in expanded hidden layers (narrow→wide→narrow), the proposed Hourglass design operates skip connections in high-dimensional spaces while computation flows through narrow bottlenecks (wide→narrow→wide).
            </p>

            <h3>Key Findings</h3>
                <ul>
                    <li><strong>Superior performance with fewer parameters:</strong> On ImageNet-32 denoising, Hourglass MLPs achieve 22.31 dB PSNR with only 66M parameters, while conventional models require 75M for equivalent performance</li>
                    
                    <li><strong>Optimal architectural pattern across all tasks:</strong> Deeper networks (L=4-5) with wider skip connections (d<sub>z</sub>≥3075) and narrower bottlenecks (d<sub>h</sub>=270-765) consistently dominate the Pareto frontier on MNIST and ImageNet-32 generative tasks</li>
                    
                    <li><strong>Training efficiency on ImageNet-32 denoising:</strong> Input projection (W<sub>in</sub>) can remain fixed at random initialization with negligible performance impact compared to trainable projection</li>
                    
                    <li><strong>Broader impact:</strong> The wide-narrow-wide design principles suggest promising future directions for Transformers and other residual networks</li>                </ul>
        </div>

        <!-- ABSTRACT -->
        <!-- <div class="section">
            <h2>Abstract</h2>
            <p>
                Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow design where skip connections operate at the input/output dimensions while processing occurs in expanded hidden spaces. We challenge this convention by proposing wide-narrow-wide (Hourglass) MLP blocks where skip connections operate at expanded dimensions while residual computation flows through narrow bottlenecks. This inversion leverages higher-dimensional spaces for incremental refinement while maintaining computational efficiency through parameter-matched designs. Implementing Hourglass MLPs requires an initial projection to lift input signals to expanded dimensions. We propose that this projection can remain fixed at random initialization throughout training, enabling efficient training and inference implementations. We evaluate both architectures on generative tasks over popular image datasets, characterizing performance-parameter Pareto frontiers through systematic architectural search. Results show that Hourglass architectures consistently achieve superior Pareto frontiers compared to conventional designs. As parameter budgets increase, optimal Hourglass configurations favor deeper networks with wider skip connections and narrower bottlenecks—a scaling pattern distinct from conventional MLPs. Our findings suggest reconsidering skip connection placement in modern architectures, with potential applications extending to Transformers and other residual networks.
            </p>
        </div> -->

        <!-- WIDE-NARROW-WIDE MLP -->
        <div class="section">
            <h2>Wide–Narrow–Wide MLP</h2>
            
            <p>
                We propose inverting the conventional narrow-wide-narrow MLP design to create wide-narrow-wide (hourglass) blocks. Based on theoretical foundations, we hypothesize that architectures with skip connections operating at higher dimensions may enable more advantageous incremental refinement.
            </p>

            <div class="figure">
                <img src="images/hourglass_arch.png" 
                     alt="Figure 1: Hourglass MLP Architecture"
                     id="figure1">
                <div class="figure-caption">
                    <strong>Figure 1:</strong> (a) Illustration of a wide-narrow-wide MLP block. The two endpoints z<sub>i</sub> and z<sub>i+1</sub> have a higher dimensionality compared to the hidden h<sub>i</sub>. (b) Illustration of a full network whose core is a stack of wide-narrow-wide MLP blocks.
                </div>
            </div>

            <p>In detail, a network built on wide-narrow-wide MLPs consists of three distinct stages:</p>

            <div class="highlight-box">
                <h4 style="margin-bottom: 10px;">1. Input-to-latent projection</h4>
                The input signal $x \in \mathbb{R}^{d_x}$, which can be a natural signal, is first projected to the latent space of $d_z$ dimensions via:
                $$z_0 = W_{\text{in}}x, \quad \text{where } W_{\text{in}} \in \mathbb{R}^{d_z \times d_x}$$
            </div>

            <div class="highlight-box">
                <h4 style="margin-bottom: 10px;">2. A stack of MLP blocks</h4>
                For block $i = 0, 1, \dots, L-1$, the incremental improvement is computed and applied in the high–dimensional space:

                $$z_{i+1} = z_i + W^H_{i,2}\,\sigma_{i}\!\left( W^H_{i,1}\,\mathrm{norm}(z_i) \right)$$
                $$\text{where} \quad W^H_{i,1} \in \mathbb{R}^{d_{h} \times d_z}, \quad W^H_{i,2} \in \mathbb{R}^{d_z \times d_h}$$
            </div>

            <div class="highlight-box">
                <h4 style="margin-bottom: 10px;">3. Output conversion</h4>
                <p>An output network $W_{\text{out}}$ converts the last latent $z_{L}$ back to the output space:
                    $$\hat{y}=W_{\text{out}}z_{L}, \quad \text{where } W_{\text{out}} \in \mathbb{R}^{d_x \times d_z}$$
                </p>
            </div>
        </div>

        <!-- EXPERIMENTS AND RESULTS -->
        <div class="section">
            <h2>Experiments and Results</h2>
            

            <h3>Tasks and Datasets</h3>
            <p>We evaluate on two image datasets with multiple generative tasks:</p>
            
            <div class="result-grid">
                <div class="result-item">
                    <h4>MNIST</h4>
                    <ul style="margin-left: 20px;">
                        <li>Generative classification</li>
                        <li>Denoising (Added noise ~$\mathcal{N}(\mu=0.5, \sigma = 0.25)$)</li>
                        <li>Super-resolution (14×14 → 28×28)</li>
                    </ul>
                </div>
                <div class="result-item">
                    <h4>ImageNet-32</h4>
                    <ul style="margin-left: 20px;">
                        <li>Denoising (Added noise ~$\mathcal{N}(\mu=0.5, \sigma = 0.25)$)</li>
                        <li>Super-resolution (16×16x3 → 32×32x3)</li>
                    </ul>
                </div>
            </div>

            <h3>Genearative Classification Task</h3>
            
            <div class="figure">
                <img src="/images/mnist_gc.png" 
                    alt="Figure 2a: Performance-complexity Pareto front"
                    style="width: 50%; height: auto;">
                <div class="figure-caption">
                    <strong>Figure 2a:</strong> Performance-complexity Pareto front.
                </div>
            </div>

            <div class="figure">
                <img src="/images/gc_demo.png" 
                    alt="Figure 2b: Samples predicted by Hourglass model"
                    style="width: 50%; height: auto;">
                <div class="figure-caption">
                    <strong>Figure 2b:</strong> Samples predicted by the proposed Hourglass model.
                </div>
            </div>

            <h3>Generative Restoration Tasks</h3>
            
            <div class="figure">
                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-bottom: 15px;">
                    <div>
                        <img src="images/mnist_denoising.png" 
                            style="width: 100%; border-radius: 12px; box-shadow: 0 5px 20px rgba(0,0,0,0.15);">
                        <p style="text-align: center; margin-top: 10px; font-size: 0.9em; color: #666;">(a) MNIST</p>
                    </div>
                    <div>
                        <img src="images/imagenet32_denoising.png"
                            style="width: 100%; border-radius: 12px; box-shadow: 0 5px 20px rgba(0,0,0,0.15);">
                        <p style="text-align: center; margin-top: 10px; font-size: 0.9em; color: #666;">(b) ImageNet-32</p>
                    </div>
                </div>
                <div class="figure-caption">
                    <strong>Figure 3:</strong> Performance-complexity Pareto fronts of Generative Restoration Task: Denoising
                </div>
            </div>

            <div class="figure">
                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-bottom: 15px;">
                    <div>
                        <img src="images/mnist_super_resolution.png" 
                            style="width: 100%; border-radius: 12px; box-shadow: 0 5px 20px rgba(0,0,0,0.15);">
                        <p style="text-align: center; margin-top: 10px; font-size: 0.9em; color: #666;">(a) MNIST</p>
                    </div>
                    <div>
                        <img src="images/imagenet32_super_resolution.png" 
                            style="width: 100%; border-radius: 12px; box-shadow: 0 5px 20px rgba(0,0,0,0.15);">
                        <p style="text-align: center; margin-top: 10px; font-size: 0.9em; color: #666;">(b) ImageNet-32</p>
                    </div>
                </div>
                <div class="figure-caption">
                    <strong>Figure 4:</strong> Performance-complexity Pareto fronts of Generative Restoration Task: Super-resolution
                </div>
            </div>


            <p>
                Across datasets and tasks, the proposed wide-narrow-wide (Hourglass) MLP consistently outperforms the conventional narrow-wide-narrow baseline. On ImageNet-32 denoising, the Hourglass model attains 22.31 dB PSNR with only 66M parameters, whereas the best conventional model requires 75M to reach the same score.
            </p>

            <h3>Pareto-Optimal Architecture Configurations</h3>
            <p>Analysis of the best-performing models reveals three consistent trends:</p>
            
            <div class="contributions">
                <ul>
                    <li><strong>Hourglass models achieve higher PSNR with fewer parameters.</strong> Across tasks, Hourglass architectures consistently surpass conventional models while using substantially fewer parameters.</li>
                    
                    <li><strong>Hourglass architectures favor depth and moderate bottlenecks.</strong> Optimal configurations typically use L = 4 or 5 with d<sub>h</sub> between 270 and 765, in contrast to conventional designs that rely on shallow depth (L ≤ 3) and very wide hidden layers (d<sub>h</sub> ≥ 3075).</li>
                    
                    <li><strong>High-dimensional skip connections improve parameter efficiency.</strong> Models with large d<sub>z</sub> (commonly 3075 or larger) and relatively small d<sub>h</sub> maintain or improve PSNR.</li>
                </ul>
            </div>
        </div>

        <div class="section">
            <h2>Effect of Fixed vs. Trainable Input Projection</h2>
    
            <div class="figure">
                <img src="images/comparison_imagenet_freeze_in.png" 
                    alt="Figure 5: Fixed vs Trainable Projection"
                    id="figure5">
                <div class="figure-caption">
                    <strong>Figure 5:</strong> Comparison between fixed and trainable input projection W<sub>in</sub> for Hourglass MLP on ImageNet-32 denoising.
                </div>
            </div>

            <p>
                On the ImageNet-32 denoising task with configuration (d<sub>z</sub>, d<sub>h</sub>, L) = (3546, 270, 5), we compare fixed versus trainable input projections. The trainable model is only marginally better than the fixed model, suggesting that the gains from learning W<sub>in</sub> are minor. Fixed projections offer a strong parameter-efficient alternative.
            </p>
        </div>


        <!-- DISCUSSIONS AND FUTURE WORK -->
        <div class="section">
            <h2>Discussions and Future Work</h2>
            
            <p>
                Our experimental results demonstrate that wide-narrow-wide (Hourglass) MLP architectures consistently outperform conventional designs across multiple generative tasks, supporting our hypothesis that skip connections at higher dimensions enable more effective incremental refinement.
            </p>

            <h3>Scaling to High-Resolution Applications</h3>
            <p>
                Due to limited computational capacity, our experiments focus on relatively low-dimensional image datasets to isolate the impact due to architectural differences. We identify two promising directions for scaling to high-resolution domains:
            </p>
            <ul>
                <li><strong>Integration with MLP-Mixer:</strong> Wide-narrow-wide blocks could be integrated into existing architectures like MLP-Mixer that maintain rich representations while keeping computational costs manageable.</li>
                
                <li><strong>Enhancement of U-Net:</strong> The Hourglass design could enhance U-Net architectures by projecting inputs into higher-dimensional latent spaces before entering the encoder-decoder pipeline.</li>
            </ul>

            <h3>Extension to Transformer Architectures</h3>
            
            <div class="figure">
                <img src="images/250924_hourglass_LLM.png" 
                     alt="Figure 7: Transformer Extension"
                     id="figure7">
                <div class="figure-caption">
                    <strong>Figure 7:</strong> Extend the wide-narrow-wide intuition to the transformer. (a) Classic transformer block. (b) Modified transformer block with wide-narrow-wide FFNs.
                </div>
            </div>

            <p>
                The wide-narrow-wide MLP architecture presents compelling opportunities for enhancing computational efficiency in modern transformer-based models. Adapting our findings to transformer architectures requires coordinated modifications across self-attention and feedforward layers:
            </p>
            
            <ul>
                <li>Feedforward layers cannot operate at expanded dimensions in isolation—the self-attention mechanism must process representations at matching wider dimensionalities</li>
                
                <li>To preserve computational efficiency, we propose incorporating efficient attention mechanisms such as Multi-Head Latent Attention, which maintains reduced attention head sizes while operating over wider representations</li>
                
                <li>Our empirical findings on deeper stacks suggest that feedforward adaptations should incorporate multiple iterative refinement blocks with wide-narrow-wide architectural pattern within each layer</li>
            </ul>

            <p>
                Such designs could enable more sophisticated representational transformations while maintaining favorable parameter-to-performance ratios, potentially advancing the state-of-the-art in efficient large-scale model architectures.
            </p>

            <!-- <div class="contributions">
                <h4 style="margin-bottom: 15px;">Key Contributions</h4>
                <ul>
                    <li>Propose inverting the conventional narrow-wide-narrow paradigm to a wide-narrow-wide (Hourglass) MLP design</li>
                    
                    <li>Demonstrate that the required input projection can be fixed at random initialization with negligible performance impact</li>
                    
                    <li>Show through empirical validation that the wide-narrow-wide design consistently leads to superior Pareto frontiers</li>
                    
                    <li>Reveal that Pareto-optimal Hourglass architectures favor deeper networks with wider skip connections and narrower bottleneck dimensions</li>
                </ul>
            </div> -->
        </div>

        <footer>
            <p>MediaTek Research &nbsp;|&nbsp; 2025</p>
        </footer>
    </div>
</body>
</html>


        <!-- ABLATION STUDY -->
        <!-- <div class="section">
            <h2>Ablation Study</h2>
            
            <p>
                To further explore the design trade-offs within the proposed wide-narrow-wide (Hourglass) MLP architecture, we conduct ablation studies focusing on two key hyperparameters: the bottleneck dimension d<sub>h</sub> and the number of residual blocks L.
            </p>

            <div class="figure">
                <img src="images/comparison_imagenet_freeze_in.png" 
                     alt="Figure 6: Ablation Study"
                     id="figure6">
                <div class="figure-caption">
                    <strong>Figure 6:</strong> Ablation study of optimal d<sub>h</sub> and L dimension for the Hourglass MLP architecture. (a) Varying bottleneck dimension. (b) Varying number of residual blocks.
                </div>
            </div> -->

            <!-- <h3>Effect of Bottleneck Width d<sub>h</sub></h3>
            <p>
                We fix the high-dimensional residual space to d<sub>z</sub> = 3546 and the number of residual blocks to L = 5, and vary the bottleneck width d<sub>h</sub>. Increasing d<sub>h</sub> improves PSNR, but the gains diminish beyond d<sub>h</sub> = 270. This suggests that moderate bottlenecks are sufficient for high performance, enabling significant parameter savings.
            </p>

            <h3>Effect of Residual Depth L</h3>
            <p>
                We fix d<sub>z</sub> = 3546, d<sub>h</sub> = 270, and vary the number of residual blocks L. Performance improves with deeper stacks, but quickly plateaus around L = 5, indicating that relatively shallow Hourglass MLPs are sufficient for strong results.
            </p> -->
        <!-- </div> -->

