<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">
    
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <title>Rethinking the Shape Convention of an MLP</title>
    <style>
        html {
            scroll-behavior: smooth;
            scroll-padding-top: 82px;  /* 導航列高度 + 一些空間 */
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            font-size: 15px;
            padding-top: 60px;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        
        header {
            background: white;
            padding: 60px 40px;
            border-radius: 16px;
            margin-bottom: 30px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.1);
            text-align: center;
        }
        
        h1 {
            font-size: 2.5em;
            color: #2c3e50;
            margin-bottom: 20px;
            line-height: 1.2;
        }
        
        .authors {
            font-size: 1.1em;
            color: #555;
            margin-bottom: 10px;
        }
        
        .affiliation {
            color: #777;
            font-size: 0.95em;
            margin-bottom: 20px;
        }
        
        .section {
            background: white;
            padding: 40px;
            border-radius: 16px;
            margin-bottom: 30px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.1);
        }
        
        h2 {
            color: #667eea;
            font-size: 2em;
            margin-bottom: 25px;
            padding-bottom: 10px;
            border-bottom: 3px solid #667eea;
        }
        
        h3 {
            color: #764ba2;
            font-size: 1.4em;
            margin: 30px 0 15px 0;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
            line-height: 1.8;
        }
                
        .figure {
            text-align: center;
            margin: 40px 0;
        }
        
        .figure img {
            max-width: 100%;
            height: auto;
            border-radius: 12px;
            box-shadow: 0 5px 20px rgba(0,0,0,0.15);
            background: #f0f0f0;
            min-height: 200px;
        }
        
        .figure-caption {
            margin-top: 15px;
            color: #666;
            font-size: 0.95em;
            font-style: italic;
            text-align: center;
        }
        
        .highlight-box {
            background: linear-gradient(135deg, #667eea15, #764ba215);
            border-left: 5px solid #667eea;
            padding: 20px;
            margin: 25px 0;
            border-radius: 8px;
        }
        
        .contributions {
            background: #fff9e6;
            padding: 25px;
            border-radius: 12px;
            border-left: 5px solid #ffc107;
            margin: 25px 0;
        }
        
        ul {
            margin-left: 20px;
            margin-top: 10px;
        }
        
        li {
            margin: 10px 0;
            line-height: 1.7;
        }
        
        .result-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }
        
        .result-item {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            border-left: 4px solid #667eea;
        }
        
        .result-item h4 {
            color: #667eea;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 0.9em;
        }
        
        th, td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }
        
        th {
            background: #667eea;
            color: white;
            font-weight: 600;
        }
        
        tr:hover {
            background: #f5f5f5;
        }
        
        footer {
            display: flex;
            justify-content: center;
            align-items: center;
            color: white;
            font-size: 1.0em;
        }
        
        @media (max-width: 768px) {
            h1 {
                font-size: 1.8em;
            }
            
            .section {
                padding: 25px;
            }
            
            header {
                padding: 40px 25px;
            }
            .hero-buttons .custom-btn {
                padding: 0.4rem 0.8rem !important;
                font-size: 14px !important;
                margin: 0.2rem !important;
            }
        }
        
        code {
            background-color: #f8f9fa;
            color: #2d2d2d;
            padding: 2px 6px;
            border-radius: 4px;
            font-weight: 500;
            font-family: 'Fira Code', monospace;
            font-size: 0.95em;
            letter-spacing: 0.5px;
            border: 1px solid #e9ecef;
        }
        
        .custom-btn {
            display: inline-block;
            padding: 0.5rem 1.2rem;
            margin: 0.3rem;
            font-size: 16px;
            text-decoration: none;
            color: white;
            background: linear-gradient(135deg, #667eea, #764ba2);
            border-radius: 8px;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);
        }

        .custom-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(102, 126, 234, 0.4);
        }

        .custom-btn i {
            margin-right: 5px;
        }

        .nav-menu {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            background: rgba(255, 255, 255, 1.00);
            backdrop-filter: blur(10px);
            padding: 18px 20px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            z-index: 1000;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .nav-logos {
            display: flex;
            gap: 20px;
            align-items: center;
        }

        .nav-logos img {
            height: 100px;
            width: auto;
            object-fit: contain;
        }

        .nav-menu ul {
            list-style: none;
            margin: 0;
            padding: 0;
            display: flex;
            justify-content: flex-end;
            align-items: center;
            gap: 30px;
        }

        .nav-menu li {
            margin: 0;
        }

        .nav-menu a {
            color: #333;
            text-decoration: none;
            font-size: 1em;
            font-weight: 500;
            transition: color 0.3s;
            padding: 0px 10px;
            border-radius: 5px;
        }

        .nav-menu a:hover {
            color: #667eea;
            background: rgba(102, 126, 234, 0.1);
        }

        @media (max-width: 768px) {
            .nav-menu {
                padding: 10px 20px;
                flex-direction: column;
                gap: 10px;
            }
            
            .nav-logos img {
                height: 28px;
            }
            
            .nav-menu ul {
                gap: 15px;
                justify-content: center;
            }
            
            .nav-menu a {
                font-size: 0.85em;
                padding: 3px 6px;
            }
        }

        @media (max-width: 768px) {
            .nav-menu ul {
                gap: 15px;
                padding: 0 10px;
            }
            
            .nav-menu a {
                font-size: 0.85em;
                padding: 3px 6px;
            }
        }

    </style>
</head>
<body>
    <div class="container">
        <nav class="nav-menu">
            <div class="nav-logos">
                <img src="images/MR-logo-3.png" style="height: 38px"; alt="MediaTek Research">
                <img src="images/ntu-logo.png" style="height: 45px"; alt="NTU">
            </div>
            <ul>
                <li><a href="#tldr">TL;DR</a></li>
                <li><a href="#hourglass_mlp">Wide-Narrow-Wide (Hourglass) MLP</a></li>
                <li><a href="#experiments">Experiments</a></li>
                <li><a href="#projection">Input Projection Analysis</a></li>
                <li><a href="#architectural">Architectural Analysis</a></li>
                <li><a href="#discussion">Discussions & Future Work</a></li>
            </ul>
        </nav>
        <header>
            <h1>Rethinking the Shape Convention of an MLP</h1>
            <div class="authors">
                Meng-Hsi Chen<sup>1*</sup>, Yu-Ang Lee<sup>1,2*</sup>, Feng-Ting Liao<sup>1</sup>, Da-shan Shiu<sup>1</sup>
            </div>
            <div class="affiliation">
                <sup>1</sup>MediaTek Research &nbsp;&nbsp;&nbsp; <sup>2</sup>National Taiwan University<br>
                <sup>*</sup>Equal contribution
            </div>
            
            <div class="correspondence" style="text-align: center; margin-top: 0.8rem; font-size: 0.9em; color: #666;">
                <i class="bi bi-envelope"></i> Correspondence: 
                <a href="mailto:meng-hsi.chen@mtkresearch.com" style="color: #2563eb; text-decoration: none;">meng-hsi.chen@mtkresearch.com</a>, 
                <a href="mailto:ft.liao@mtkresearch.com" style="color: #2563eb; text-decoration: none;">ft.liao@mtkresearch.com</a>
            </div>
            <div class="hero-buttons" style="text-align:center; margin-top: 1rem;">
            <a href="https://arxiv.org/abs/2510.01796" class="custom-btn">
                <i class="bi bi-file-pdf"></i> Paper
            </a>
            <a href="" class="custom-btn">
                <i class="bi bi-github"></i> Code
            </a>
            <a href="https://huggingface.co/papers/2510.01796" class="custom-btn">
                <img src="images/huggingface.svg" alt="HuggingFace" style="height: 20px; width: auto; margin-right: 5px; vertical-align: middle;"> HuggingFace
            </a>
        </div>
        </header>

        <!-- TL;DR -->
       <div class="section" id="tldr">
        <h2>TL;DR</h2>
        
        <!-- <h3>Motivation and Method</h3> -->
        <div class="highlight-box" style="background: linear-gradient(135deg, #667eea10, #764ba210); border-left: 4px solid #667eea;">
            <ul style="list-style-type: disc; margin-left: 20px;">
                <li style="font-size: 1.0em; font-weight: 500; color: #2c3e50; margin-bottom: 15px;">
                    Building upon theoretical frameworks suggesting computational advantages in higher-dimensional spaces (e.g., Cover's theorem on linear separability, kernel methods in SVMs, and reservoir computing with fixed random projections), we invert the standard MLP paradigm.
                </li>
                <li style="font-size: 1.0em; font-weight: 500; color: #2c3e50; margin-bottom: 15px;">
                    Instead of the conventional "narrow→wide→narrow" architectures with skip connections operating at the narrower input/output dimensions, our <strong>Hourglass design</strong> employs "wide→narrow→wide" blocks where skip connections operate in expanded latent dimensions while residual computations flow through narrow bottlenecks.
                </li>
                <li style="font-size: 1.0em; font-weight: 500; color: #2c3e50; margin-bottom: 15px;">        
                     Implementing Hourglass MLPs requires an initial projection to lift input signals to expanded dimensions. We propose that this projection can remain fixed at random initialization throughout training, enabling efficient training and inference implementations.
                </li>
                
                <li style="font-size: 1.0em; font-weight: 500; color: #2c3e50; margin-bottom: 15px;">
                    Through empirical validation on generative tasks over popular image datasets, we show that the wide-narrow-wide design consistently leads to superior Performance-complexity Pareto frontiers.
                </li>    
            </ul>
        </div>
    </div>

        <!-- WIDE-NARROW-WIDE MLP -->
        <div class="section" id="hourglass_mlp">
            <h2>Wide-Narrow-Wide (Hourglass) MLP</h2>
            
            <p>
                <!-- We propose inverting the conventional narrow-wide-narrow MLP design to create wide-narrow-wide (hourglass) blocks.  -->
                Based on theoretical foundations, we hypothesize that architectures with skip connections operating at higher dimensions may enable more advantageous incremental refinement.
            </p>

            <div class="figure">
                <img src="images/hourglass_arch.png" 
                     alt="Figure 1: Hourglass MLP Architecture"
                     style="width: 60%; border-radius: 12px; box-shadow: 0 5px 20px rgba(0,0,0,0.15)"
                     id="figure1">
                <div class="figure-caption">
                    <strong>Figure 1:</strong> (a) Illustration of a wide-narrow-wide MLP block. The two endpoints z<sub>i</sub> and z<sub>i+1</sub> have a higher dimensionality compared to the hidden h<sub>i</sub>. <P></P>
                    (b) Illustration of a full network whose core is a stack of wide-narrow-wide MLP blocks.
                </div>
            </div>

            <p>In detail, a network built on wide-narrow-wide MLPs consists of three distinct stages:</p>

            <div class="highlight-box">
                <h4 style="margin-bottom: 10px;">1. Input-to-latent projection</h4>
                The input signal $x \in \mathbb{R}^{d_x}$, which can be a natural signal, is first projected to the latent space of $d_z$ dimensions via:
                $$z_0 = W_{\text{in}}x, \quad \text{where } W_{\text{in}} \in \mathbb{R}^{d_z \times d_x}$$
            </div>

            <div class="highlight-box">
                <h4 style="margin-bottom: 10px;">2. A stack of MLP blocks</h4>
                For block $i = 0, 1, \dots, L-1$, the incremental improvement is computed and applied in the high–dimensional space:

                $$z_{i+1} = z_i + W^H_{i,2}\,\sigma_{i}\!\left( W^H_{i,1}\,\mathrm{norm}(z_i) \right)$$
                $$\text{where} \quad W^H_{i,1} \in \mathbb{R}^{d_{h} \times d_z}, \quad W^H_{i,2} \in \mathbb{R}^{d_z \times d_h}$$
            </div>

            <div class="highlight-box">
                <h4 style="margin-bottom: 10px;">3. Output conversion</h4>
                <p>An output network $W_{\text{out}}$ converts the last latent $z_{L}$ back to the output space:
                    $$\hat{y}=W_{\text{out}}z_{L}$$
                    Note that the shape of $W_{\text{out}}$ depends on the task-specific output dimension (e.g., for denoising, $W_{\text{out}} \in \mathbb{R}^{d_x \times d_z}$).                </p>            
            </div>
        </div>

        <!-- EXPERIMENTS AND RESULTS -->
        <div class="section" id="experiments">
            <h2>Experiments</h2>
            

            <h3>Setup</h3>
            <p>We evaluate on two image datasets with multiple generative tasks:</p>
            
            
            <div class="result-grid">
                <div class="result-item">
                    <h4>MNIST</h4>
                    <ul style="margin-left: 20px;">
                        <li>Generative classification</li>
                        <li>Denoising (Added noise ~$\mathcal{N}(0, 0.25)$)</li>
                        <li>Super-resolution (14×14 → 28×28)</li>
                    </ul>
                </div>
                <div class="result-item">
                    <h4>ImageNet-32</h4>
                    <ul style="margin-left: 20px;">
                        <li>Denoising (Added noise ~$\mathcal{N}(0, 0.25)$)</li>
                        <li>Super-resolution (16×16x3 → 32×32x3)</li>
                    </ul>
                </div>
            </div>

            <h3>Generative Classification Task</h3>
            
            <div class="figure">
                <img src="images/gc_demo.png" 
                    style="width: 40%; height: auto;">
                <div class="figure-caption">
                    <strong>Figure 2a:</strong> Generative classification task on MNIST requires a model to take in an input digit image and generates a prototypical digit image.
                    <!-- Samples predicted by the proposed Hourglass model. -->
                </div>
            </div>

            <div class="figure">
                <img src="images/mnist_gc.png" 
                    style="width: 25%; height: auto;">
                <div class="figure-caption">
                    <strong>Figure 2b:</strong> Performance-complexity Pareto front of Generative Classification Task on MNIST.
                </div>
            </div>


            <h3>Generative Restoration Tasks</h3>
            
            <div class="figure">
                <div style="display: flex; justify-content: center; gap: 30px; margin-bottom: 15px; align-items: flex-start;">
                    <div style="text-align: center;">
                        <img src="images/mnist_denoising.png" 
                            style="width: 280px; max-width: 100%; border-radius: 12px; box-shadow: 0 5px 20px rgba(0,0,0,0.15);">
                        <p style="text-align: center; margin-top: 10px; font-size: 0.9em; color: #666;">(a) MNIST</p>
                    </div>
                    <div style="text-align: center;">
                        <img src="images/imagenet32_denoising.png"
                            style="width: 280px; max-width: 100%; border-radius: 12px; box-shadow: 0 5px 20px rgba(0,0,0,0.15);">
                        <p style="text-align: center; margin-top: 10px; font-size: 0.9em; color: #666;">(b) ImageNet-32</p>
                    </div>
                </div>
                <div class="figure-caption">
                    <strong>Figure 3:</strong> Performance-complexity Pareto fronts of Generative Restoration Task: Denoising
                </div>
            </div>

            <div class="figure">
                    <div style="display: flex; justify-content: center; gap: 30px; margin-bottom: 15px; align-items: flex-start;">
                        <div style="text-align: center;">
                            <img src="images/mnist_super_resolution.png" 
                                style="width: 280px; max-width: 100%; border-radius: 12px; box-shadow: 0 5px 20px rgba(0,0,0,0.15);">
                            <p style="text-align: center; margin-top: 10px; font-size: 0.9em; color: #666;">(a) MNIST</p>
                        </div>
                        <div style="text-align: center;">
                            <img src="images/imagenet32_super_resolution.png" 
                                style="width: 280px; max-width: 100%; border-radius: 12px; box-shadow: 0 5px 20px rgba(0,0,0,0.15);">
                            <p style="text-align: center; margin-top: 10px; font-size: 0.9em; color: #666;">(b) ImageNet-32</p>
                        </div>
                    </div>
                    <div class="figure-caption">
                        <strong>Figure 4:</strong> Performance-complexity Pareto fronts of Generative Restoration Task: Super-resolution
                    </div>
            </div>
            

            <h3>Observation</h3>
            
            <div class="contributions">
                <ul>
                    <li><strong>Superior performance with fewer parameters.</strong> Across datasets and tasks, the proposed wide-narrow-wide (Hourglass) MLP consistently outperforms the conventional narrow-wide-narrow baseline. Specifically, on ImageNet-32 denoising, the Hourglass model attains 22.31 dB PSNR with only 66M parameters, whereas the best conventional model requires 75M to reach the same score.</li>
                    
                    <li><strong>Deeper networks with moderate bottlenecks.</strong> Optimal Hourglass configurations typically use L = 4 or 5 with d<sub>h</sub> between 270 and 765, in contrast to conventional designs that rely on shallow depth (L ≤ 3) and very wide hidden layers (d<sub>h</sub> ≥ 3075).</li>
                    
                    <li><strong>High-dimensional skip connections improve parameter efficiency.</strong> Models with large d<sub>z</sub> (commonly 3075 or larger) and relatively small d<sub>h</sub> maintain or improve PSNR, demonstrating the effectiveness of operating skip connections in expanded latent dimensions.</li>
                </ul>
            </div>
        </div>

        <div class="section" id="projection">
            <h2>Effect of Fixed vs. Trainable Input Projection</h2>
    
            <div class="figure">
                <img src="images/comparison_imagenet_freeze_in.png" 
                    alt="Figure 5: Fixed vs Trainable Projection"
                     style="width: 60%; border-radius: 12px; box-shadow: 0 5px 20px rgba(0,0,0,0.15);"
                    id="figure5">
                <div class="figure-caption">
                    <strong>Figure 5:</strong> Comparison between fixed and trainable input projection W<sub>in</sub> for Hourglass MLP with (d<sub>z</sub>, d<sub>h</sub>, L) = (3546, 270, 5) on ImageNet-32 denoising.
                </div>
            </div>
            
            <h3>Observation</h3>
            <div class="contributions">
                <ul>
                    <li>Implementing Hourglass MLPs requires an initial projection ($W_{\text{in}}$) to lift inputs to expanded dimensions. Inspired by reservoir computing, we propose that this projection can remain fixed at random initialization throughout training.</li>
                    <li>Figure 5 demonstrates that fixing $W_{\text{in}}$ at random initialization performs comparably to training it on ImageNet-32 denoising, which can offset the additional burden of having to carry one more matrix-vector computing layer.</li>
                </ul>
            </div>
        </div>

        <div class="section" id="architectural">
            <h2>Hourglass Architectural Analysis</h2>
            To further explore the design trade-offs within the proposed Hourglass MLP architecture, we conduct ablation studies focusing on two key hyperparameters: the bottleneck dimension $d_h$ and the number of residual blocks $L$.
            <div class="figure">
                    <div style="display: flex; justify-content: center; gap: 30px; margin-bottom: 15px; align-items: flex-start;">
                        <div style="text-align: center;">
                            <img src="images/vary_dh.png" 
                                style="width: 350px; max-width: 100%; border-radius: 12px; box-shadow: 0 5px 20px rgba(0,0,0,0.15);">
                            <p style="text-align: center; margin-top: 10px; font-size: 0.9em; color: #666;">(a) Varying the bottleneck dimension $d_h$</p>
                        </div>
                        <div style="text-align: center;">
                            <img src="images/vary_L.png" 
                                style="width: 350px; max-width: 100%; border-radius: 12px; box-shadow: 0 5px 20px rgba(0,0,0,0.15);">
                            <p style="text-align: center; margin-top: 10px; font-size: 0.9em; color: #666;">(b) Varying the number of residual blocks $L$</p>
                        </div>
                    </div>
                    <div class="figure-caption">
                        <strong>Figure 6:</strong> Ablation study of optimal $d_h$ and $L$ dimension for the Hourglass MLP architecture.
                    </div>
            </div>
            
            


            <h3>Observation</h3>
            <div class="contributions">
                <ul>
                    <li>Moderate bottlenecks are sufficient for high performance, enabling significant parameter savings.</li>
                    <li>Performance improves with deeper stacks, but quickly plateaus around $L=5$, indicating that relatively shallow Hourglass MLPs are sufficient for strong results.</li>
                </ul>
            </div>
        </div>


        <!-- DISCUSSIONS AND FUTURE WORK -->
        <div class="section" id="discussion">
            <h2>Discussions and Future Work</h2>
            
           
            <h3>Scaling to High-Resolution Applications</h3>
            <p>Due to limited computational capacity, our experiments primarily focus on relatively low-dimensional image datasets.</p>
            <p>We identify two promising directions for scaling to high-resolution domains:</p>
            
            <div class="highlight-box" style="background: linear-gradient(135deg, #667eea10, #764ba210); border-left: 4px solid #667eea;">
                <ul style="list-style-type: disc; margin-left: 20px;">
                    <li style="font-size: 1.0em; font-weight: 500; color: #2c3e50; margin-bottom: 15px;">
                        <strong>Integration with MLP-Mixer:</strong> Wide-narrow-wide blocks could be integrated into existing architectures like MLP-Mixer that maintain rich representations while keeping computational costs manageable.
                    </li>
                    <li style="font-size: 1.0em; font-weight: 500; color: #2c3e50; margin-bottom: 15px;">
                        <strong>Enhancement of U-Net:</strong> The Hourglass design could enhance U-Net architectures by projecting inputs into higher-dimensional latent spaces before entering the encoder-decoder pipeline.
                    </li>
                </ul>
            </div>

            <h3>Extension to Transformer Architectures</h3>
            <p>
                The wide-narrow-wide MLP architecture presents compelling opportunities for enhancing computational efficiency in modern transformer-based models. 
            </p>
            <div class="figure">
                <img src="images/250924_hourglass_LLM.png" 
                     alt="Figure 7: Transformer Extension"
                     style="width: 60%; border-radius: 12px; box-shadow: 0 5px 20px rgba(0,0,0,0.15);"
                     id="figure7">
                <div class="figure-caption">
                    <strong>Figure 7:</strong> Extend the wide-narrow-wide intuition to the transformer. (a) Classic transformer block. (b) Modified transformer block with wide-narrow-wide FFNs.
                </div>
            </div>


            <p>
                Adapting our findings to transformer architectures requires coordinated modifications across self-attention and feedforward layers:
            </p>
            
            
            <div class="highlight-box" style="background: linear-gradient(135deg, #667eea10, #764ba210); border-left: 4px solid #667eea;">
                <ul style="list-style-type: disc; margin-left: 20px;">
                    <li style="font-size: 1.0em; font-weight: 500; color: #2c3e50; margin-bottom: 15px;">
                        Feedforward layers cannot operate at expanded dimensions in isolation—the self-attention mechanism must process representations at matching wider dimensionalities
                    </li>
                    <li style="font-size: 1.0em; font-weight: 500; color: #2c3e50; margin-bottom: 15px;">
                        To preserve computational efficiency, we propose incorporating efficient attention mechanisms such as Multi-Head Latent Attention, which maintains reduced attention head sizes while operating over wider representations
                    </li>
                    <li style="font-size: 1.0em; font-weight: 500; color: #2c3e50; margin-bottom: 15px;">
                        Our empirical findings on deeper stacks suggest that feedforward adaptations should incorporate multiple iterative refinement blocks with wide-narrow-wide architectural pattern within each layer
                    </li>
                </ul>
            </div>
        </div>

        <footer>
            <p>MediaTek Research &nbsp;|&nbsp; 2025</p>
        </footer>
    </div>
</body>
</html>
       
